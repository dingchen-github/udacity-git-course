{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow object called tensor\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(10)\n",
    "x = tf.Variable(5)\n",
    "x = tf.add(5, 2)  # 7\n",
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5)  # 10\n",
    "z = tf.divide(10, 2) # 5\n",
    "tf.matmul(a, b) # matrix multiplication, the order of a and b matters!\n",
    "# cast a float to integer\n",
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from test import *\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels/classes\n",
    "n_classes = 10\n",
    "\n",
    "# Features and Labels\n",
    "# Single-precision floating-point format, usually occupying 32 bits (4 bytes) in computer memory\n",
    "# None for variable batch sizes, because the last batch may be smaller.\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights and Biases\n",
    "# use the tf.truncated_normal() function to generate random numbers from a normal distribution.\n",
    "# goal is to get 0 mean and small equal variance.\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Training set.\n",
    "### train_features, train_labels = mnist_features_labels(n_labels)\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True, reshape=False)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "### prediction = tf.nn.softmax(logits)\n",
    "# Cross entropy. Output is one-hot encoding.\n",
    "### cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "# Average cross entropy over the entire training set\n",
    "### cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Calculate accuracy\n",
    "# Reducing loss/cost means increasing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Two hidden layers! --- ###\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# The file path to save the data. The \".ckpt\" extension stands for \"checkpoint\".\n",
    "save_file = './model.ckpt'\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1]) # features\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) # labels\n",
    "# reshapes the 28px by 28px matrices in x into row vectors of 784px.\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "# Size/width of the hidden layer\n",
    "n_hidden_layer = 256\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer]), name='weights_0'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]), name='bias_0')\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Dropout. Probability to keep units\n",
    "# During training, a good starting value for keep_prob is 0.5.\n",
    "# During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Hidden layer with RELU activation function and dropout\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "# Output layer with linear activation function\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "# Reducing loss/cost means increasing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits=logits, 1), tf.argmax(labels=y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias. Don't need to call tf.global_variables_initializer()\n",
    "    ### saver.restore(sess, save_file)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob = 0.5})\n",
    "    \n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict={features: mnist.validation.images, labels: mnist.validation.labels, keep_prob = 1.0})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(epoch, valid_accuracy))\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- CNN --- ###\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# The file path to save the data. The \".ckpt\" extension stands for \"checkpoint\".\n",
    "save_file = './model.ckpt'\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 1 # input depth\n",
    "\n",
    "# Output depth\n",
    "k_output1 = 32\n",
    "k_output2 = 64\n",
    "k_output3 = 1024\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Dropout. Probability to keep units\n",
    "dropout = 0.75\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1]) # features\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) # labels\n",
    "# input = tf.placeholder(tf.float32, shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias for only 1 convolutional layer\n",
    "# filter_weight = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "# filter_bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Store layers weight & bias for multiple layers\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([filter_size_height, filter_size_width, color_channels, k_output1])),\n",
    "    'wc2': tf.Variable(tf.random_normal([filter_size_height, filter_size_width, k_output1, k_output2])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*k_output2, k_output3])),\n",
    "    'out': tf.Variable(tf.random_normal([k_output3, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([k_output1])),\n",
    "    'bc2': tf.Variable(tf.random_normal([k_output2])),\n",
    "    'bd1': tf.Variable(tf.random_normal([k_output3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "# the first element in this array indicates the stride for batch and last element indicates stride for features. \n",
    "# It's good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them.\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "\n",
    "# SAME Padding, the output height and width are computed as:\n",
    "# out_height = ceil(float(in_height) / float(strides[1]))\n",
    "# out_width = ceil(float(in_width) / float(strides[2]))\n",
    "# VALID Padding, the output height and width are computed as:\n",
    "# out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "# out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "# padding = 'SAME'\n",
    "\n",
    "# Convolution\n",
    "def conv2d(x, W, b, stride=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# Apply Max Pooling. 2x2 filters with a stride of 2x2 are common in practice.\n",
    "def maxpool2d(x, stride=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "# Layer 1 - 28*28*1 to 28*28*32 to 14*14*32\n",
    "conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "conv1 = maxpool2d(conv1)\n",
    "# Layer 2 - 14*14*32 to 14*14*64 to 7*7*64\n",
    "conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "conv2 = maxpool2d(conv2)\n",
    "\n",
    "# Fully connected layer - 7*7*64 to 1024\n",
    "fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]]) # reshape the 7*7*64 matrices into row vectors of 1024 px.\n",
    "# Or use flatten.\n",
    "# from tensorflow.contrib.layers import flatten\n",
    "# fc1 = tf.contrib.layers.flatten(conv2)\n",
    "fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output Layer - class prediction - 1024 to 10\n",
    "logits = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Adam optimizer\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "# Reducing loss/cost means increasing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits=logits, 1), tf.argmax(labels=y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias. Don't need to call tf.global_variables_initializer()\n",
    "    ### saver.restore(sess, save_file)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "    \n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            # loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict={features: mnist.validation.images[:test_valid_size], labels: mnist.validation.labels[:test_valid_size], keep_prob = 1.0})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(epoch, valid_accuracy))\n",
    "            # print('Epoch {:>2}, Batch {:>3} - Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(epoch + 1, batch + 1, loss, valid_accuracy))\n",
    "    \n",
    "    # Calculate Test Accuracy\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images[:test_valid_size], y: mnist.test.labels[:test_valid_size], keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_accuracy))\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting to show that your trained network's weights are looking for interesting features, \n",
    "# whether it's looking at differences in feature maps from images with or without a sign, \n",
    "# or even what feature maps look like in a trained network vs a completely untrained one on the same sign image.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer.\n",
    "# for instance if you wanted to see what the LeNet lab's feature maps looked like for it's second convolutional layer,\n",
    "# you could enter conv2 as the tf_activation variable.\n",
    "# Note: that to get access to tf_activation, the session should be interactive which can be achieved with the following commands.\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.as_default()\n",
    "\n",
    "# activation_min/max: can be used to view the activation contrast in more detail,\n",
    "# by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects with size, normalization, ect if needed\n",
    "    # image_input = \n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it maybe having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x: image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
