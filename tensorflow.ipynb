{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow object called tensor\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(10)\n",
    "x = tf.Variable(5)\n",
    "x = tf.add(5, 2)  # 7\n",
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5)  # 10\n",
    "z = tf.divide(10, 2) # 5\n",
    "tf.matmul(a, b) # matrix multiplication, the order of a and b matters!\n",
    "# cast a float to integer\n",
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from test import *\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels/classes\n",
    "n_classes = 10\n",
    "\n",
    "# Features and Labels\n",
    "# Single-precision floating-point format, usually occupying 32 bits (4 bytes) in computer memory\n",
    "# None for variable batch sizes, because the last batch may be smaller.\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights and Biases\n",
    "# use the tf.truncated_normal() function to generate random numbers from a normal distribution.\n",
    "# goal is to get 0 mean and small equal variance.\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Training set.\n",
    "### train_features, train_labels = mnist_features_labels(n_labels)\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True, reshape=False)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "### prediction = tf.nn.softmax(logits)\n",
    "# Cross entropy. Output is one-hot encoding.\n",
    "### cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "# Average cross entropy over the entire training set\n",
    "### cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Calculate accuracy\n",
    "# Reducing loss/cost means increasing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Two hidden layers! --- ###\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# The file path to save the data. The \".ckpt\" extension stands for \"checkpoint\".\n",
    "save_file = './model.ckpt'\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1]) # features\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) # labels\n",
    "# reshapes the 28px by 28px matrices in x into row vectors of 784px.\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "# Size/width of the hidden layer\n",
    "n_hidden_layer = 256\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer]), name='weights_0'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]), name='bias_0')\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Dropout. Probability to keep units\n",
    "# During training, a good starting value for keep_prob is 0.5.\n",
    "# During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Hidden layer with RELU activation function and dropout\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "# Output layer with linear activation function\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "# Reducing loss/cost means increasing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits=logits, 1), tf.argmax(labels=y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias. Don't need to call tf.global_variables_initializer()\n",
    "    ### saver.restore(sess, save_file)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob = 0.5})\n",
    "    \n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict={features: mnist.validation.images, labels: mnist.validation.labels, keep_prob = 1.0})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(epoch, valid_accuracy))\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- CNN --- ###\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# The file path to save the data. The \".ckpt\" extension stands for \"checkpoint\".\n",
    "save_file = './model.ckpt'\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 1 # input depth\n",
    "\n",
    "# Output depth\n",
    "k_output1 = 32\n",
    "k_output2 = 64\n",
    "k_output3 = 1024\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Dropout. Probability to keep units\n",
    "dropout = 0.75\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1]) # features\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) # labels\n",
    "# input = tf.placeholder(tf.float32, shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias for only 1 convolutional layer\n",
    "# filter_weight = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "# filter_bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Store layers weight & bias for multiple layers\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([filter_size_height, filter_size_width, color_channels, k_output1])),\n",
    "    'wc2': tf.Variable(tf.random_normal([filter_size_height, filter_size_width, k_output1, k_output2])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*k_output2, k_output3])),\n",
    "    'out': tf.Variable(tf.random_normal([k_output3, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([k_output1])),\n",
    "    'bc2': tf.Variable(tf.random_normal([k_output2])),\n",
    "    'bd1': tf.Variable(tf.random_normal([k_output3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "# the first element in this array indicates the stride for batch and last element indicates stride for features. \n",
    "# It's good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them.\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "\n",
    "# SAME Padding, the output height and width are computed as:\n",
    "# out_height = ceil(float(in_height) / float(strides[1]))\n",
    "# out_width = ceil(float(in_width) / float(strides[2]))\n",
    "# VALID Padding, the output height and width are computed as:\n",
    "# out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "# out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "# padding = 'SAME'\n",
    "\n",
    "# Convolution\n",
    "def conv2d(x, W, b, stride=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# Apply Max Pooling. 2x2 filters with a stride of 2x2 are common in practice.\n",
    "def maxpool2d(x, stride=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "# Layer 1 - 28*28*1 to 28*28*32 to 14*14*32\n",
    "conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "conv1 = maxpool2d(conv1)\n",
    "# Layer 2 - 14*14*32 to 14*14*64 to 7*7*64\n",
    "conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "conv2 = maxpool2d(conv2)\n",
    "\n",
    "# Fully connected layer - 7*7*64 to 1024\n",
    "fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]]) # reshape the 7*7*64 matrices into row vectors of 1024 px.\n",
    "# Or use flatten.\n",
    "# from tensorflow.contrib.layers import flatten\n",
    "# fc1 = tf.contrib.layers.flatten(conv2)\n",
    "fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output Layer - class prediction - 1024 to 10\n",
    "logits = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Adam optimizer\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "# Reducing loss/cost means increasing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits=logits, 1), tf.argmax(labels=y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias. Don't need to call tf.global_variables_initializer()\n",
    "    ### saver.restore(sess, save_file)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "    \n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            # loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict={features: mnist.validation.images[:test_valid_size], labels: mnist.validation.labels[:test_valid_size], keep_prob = 1.0})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(epoch, valid_accuracy))\n",
    "            # print('Epoch {:>2}, Batch {:>3} - Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(epoch + 1, batch + 1, loss, valid_accuracy))\n",
    "    \n",
    "    # Calculate Test Accuracy\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images[:test_valid_size], y: mnist.test.labels[:test_valid_size], keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_accuracy))\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keras ###\n",
    "# JUPYTER workspaces, using python 3.5, Tensorflow 1.3, and Keras 2.09.\n",
    "# http://faroit.com/keras-docs/2.0.9/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Lambda, Cropping2D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "num_classes = 5\n",
    "input_shape = (32,32,3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#####################\n",
    "\n",
    "# preprocess data\n",
    "X_normalized = np.array(X_train / 255.0 - 0.5 ) # or use model.add(Lambda)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot = label_binarizer.fit_transform(y_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25)) # dropout=0.25 in Keras equals keep_prob=0.75 in Tensorflow\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_normalized, y_one_hot, batch_size=batch_size, epochs=20, verbose=1, callbacks=[checkpoint, stopper], validation_split=0.2, \n",
    "                    shuffle=True)\n",
    "\n",
    "#############################\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'your_image.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# Take over the original weights. Do not include the top (last) fully connected layer, but use an own layer for your output\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Perform inference on our pre-processed image\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Check the top 3 predictions of the model\n",
    "print('Predicted:', decode_predictions(predictions, top=3)[0])\n",
    "\n",
    "\n",
    "###\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "###\n",
    "# Whole-model saving (configuration + weights)\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### InceptionV3\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "img = image.load_img(img_path, target_size=(299, 299))\n",
    "model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# We can use smaller than the default 299x299x3 input for InceptionV3\n",
    "# which will speed up training. Keras v2.0.9 supports down to 139x139x3\n",
    "input_size = 139\n",
    "# Using Inception with ImageNet pre-trained weights\n",
    "inception = InceptionV3(weights='imagenet', include_top=False,\n",
    "                        input_shape=(input_size,input_size,3))\n",
    "\n",
    "# Freeze the weights (use the pre-trained weights)\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "inception.summary()\n",
    "\n",
    "# Transfer learning for CIFAR-10\n",
    "from keras.layers import Input, Lambda\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Makes the input placeholder layer 32x32x3 for CIFAR-10\n",
    "cifar_input = Input(shape=(32,32,3))\n",
    "\n",
    "# Re-sizes the input with Kera's Lambda layer & attach to cifar_input\n",
    "resized_input = Lambda(lambda image: tf.image.resize_images( \n",
    "    image, (input_size, input_size)))(cifar_input)\n",
    "\n",
    "# Feeds the re-sized input into Inception model\n",
    "inp = inception(resized_input)\n",
    "\n",
    "## Setting `include_top` to False earlier also removed the GlobalAveragePooling2D layer, but we still want it.\n",
    "x = GlobalAveragePooling2D()(inp)\n",
    "# Create two new fully-connected layers\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "predictions = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "# Imports the Model API\n",
    "from keras.models import Model\n",
    "\n",
    "# Creates the model, assuming your final layer is named \"predictions\"\n",
    "model = Model(inputs=cifar_input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Check the summary of this new model to confirm the architecture\n",
    "model.summary()\n",
    "\n",
    "# Add callbacks\n",
    "checkpoint = ModelCheckpoint(filepath=save_path, monitor='val_loss', save_best_only=True)\n",
    "stopper = EarlyStopping(monitor='val_acc', min_delta=0.0003, patience=5)\n",
    "model.fit(callbacks=[checkpoint, stopper])\n",
    "\n",
    "# Load data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
    "\n",
    "# One-hot encode the labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot_train = label_binarizer.fit_transform(y_train)\n",
    "y_one_hot_val = label_binarizer.fit_transform(y_val)\n",
    "\n",
    "# Shuffle the training & test data\n",
    "X_train, y_one_hot_train = shuffle(X_train, y_one_hot_train)\n",
    "X_val, y_one_hot_val = shuffle(X_val, y_one_hot_val)\n",
    "\n",
    "# We are only going to use the first 10,000 images for speed reasons\n",
    "# And only the first 2,000 images from the test set\n",
    "X_train = X_train[:10000]\n",
    "y_one_hot_train = y_one_hot_train[:10000]\n",
    "X_val = X_val[:2000]\n",
    "y_one_hot_val = y_one_hot_val[:2000]\n",
    "\n",
    "# Use a generator to pre-process our images for ImageNet\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "if preprocess_flag == True:\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "else:\n",
    "    datagen = ImageDataGenerator()\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "# Note: we aren't using callbacks here since we only are using 5 epochs to conserve GPU time\n",
    "model.fit_generator(datagen.flow(X_train, y_one_hot_train, batch_size=batch_size), \n",
    "                    steps_per_epoch=len(X_train)/batch_size, epochs=epochs, verbose=1, \n",
    "                    validation_data=val_datagen.flow(X_val, y_one_hot_val, batch_size=batch_size),\n",
    "                    validation_steps=len(X_val)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python drive.py model.h5\n",
    "python drive.py model.h5 run1\n",
    "python video.py run1\n",
    "\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import csv\n",
    "\n",
    "samples = []\n",
    "with open('./driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = './IMG/'+batch_sample[0].split('/')[-1]\n",
    "                center_image = ndimage.imread(name)\n",
    "                # imgBGR = cv2.imread(name)\n",
    "                # imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)\n",
    "                center_angle = float(batch_sample[3])\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "#             steering_center = float(row[3])\n",
    "\n",
    "#             # create adjusted steering measurements for the side camera images\n",
    "#             correction = 0.2 # this is a parameter to tune\n",
    "#             steering_left = steering_center + correction\n",
    "#             steering_right = steering_center - correction\n",
    "\n",
    "#             # read in images from center, left and right cameras\n",
    "#             path = \"...\" # fill in the path to your training IMG directory\n",
    "#             img_center = process_image(np.asarray(Image.open(path + row[0])))\n",
    "#             img_left = process_image(np.asarray(Image.open(path + row[1])))\n",
    "#             img_right = process_image(np.asarray(Image.open(path + row[2])))\n",
    "\n",
    "#             # add images and angles to data set\n",
    "#             car_images.extend(img_center, img_left, img_right)\n",
    "#             steering_angles.extend(steering_center, steering_left, steering_right)\n",
    "            \n",
    "# augmented_images, augmented_measurements = [],[]\n",
    "# for image, measurement in zip(images, measurements):\n",
    "#     augmented_images.append(image)\n",
    "#     augmented_measurements.append(measurement)\n",
    "#     augmented_images.append(cv2.flig(image,1)) # image_flipped = np.fliplr(image)\n",
    "#     augmented_measurements.append(measurement*-1.0)\n",
    "# X_train = np.array(augmented_images)\n",
    "# y_train = np.array(augmented_measurements)\n",
    "\n",
    "# Set our batch size\n",
    "batch_size=32\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n",
    "ch, row, col = 3, 160, 320  # Trimmed image format\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Lambda(lambda x: x/127.5 - 1., input_shape=(ch, row, col), output_shape=(ch, row, col)))\n",
    "model.add(Cropping2D(cropping = ((70,25),(0,0)))) # crop top 70 and bottom 25 pixels, left 0 and right 0\n",
    "model.compile(loss='mse',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch = ceil(len(train_samples)/batch_size), validation_data = validation_generator,\n",
    "    nb_val_samples = ceil(len(validation_samples)/batch_size),\n",
    "    nb_epoch=5, verbose=1)\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
